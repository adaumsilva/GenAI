{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq65cvHruAx9PSoZqrsc5/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adaumsilva/GenAI/blob/main/translation_with_sequence_to_sequence_RNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot8Xpus_AoMb"
      },
      "outputs": [],
      "source": [
        "# Description: Sequence-to-sequence to implement an RNN-based model for a translation task using PyTorch.\n",
        "# Author: Adam Silva\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%pip install numpy==1.26.4 pandas matplotlib seaborn scikit-learn portalocker==2.8.2 nltk spacy\n",
        "%pip install torch==2.3.0+cpu torchdata==0.9.0+cpu torchtext==0.18.0+cpu \\\n",
        "    --index-url https://download.pytorch.org/whl/cpu"
      ],
      "metadata": {
        "id": "SLycMhPxBEG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "dSmnw6yABMPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " W_xh=torch.tensor(-10.0)\n",
        " W_hh=torch.tensor(10.0)\n",
        " b_h=torch.tensor(0.0)\n",
        " x_t=1\n",
        " h_prev=torch.tensor(-1)"
      ],
      "metadata": {
        "id": "YUYjT_lUBoCZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=[1,1,-1,-1,1,1]"
      ],
      "metadata": {
        "id": "EYi-W2YFCKTc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H=[-1,-1,0,1,0,-1]"
      ],
      "metadata": {
        "id": "n_O1cko6CRPF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store the predicted state values\n",
        "H_hat = []\n",
        "# Loop through each data point in the input sequence X\n",
        "t=1\n",
        "for x in X:\n",
        "    # Assign the current data point to x_t\n",
        "    print(\"t=\",t)\n",
        "    x_t = x\n",
        "    # Print the value of the previous state (h at time t-1)\n",
        "    print(\"h_t-1\", h_prev.item())\n",
        "\n",
        "    # Compute the current state (h at time t) using the RNN formula with tanh activation\n",
        "    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)\n",
        "\n",
        "    # Update h_prev to the current state value for the next iteration\n",
        "    h_prev = h_t\n",
        "\n",
        "    # Print the current input value (x at time t)\n",
        "    print(\"x_t\", x_t)\n",
        "\n",
        "    # Print the computed state value (h at time t)\n",
        "    print(\"h_t\", h_t.item())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Append the current state value to the H_hat list after converting it to integer\n",
        "    H_hat.append(int(h_t.item()))\n",
        "    t+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZDO7s8WCkCm",
        "outputId": "2be7cdce-7267-483f-a833-56f0951381a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t= 1\n",
            "h_t-1 -1\n",
            "x_t 1\n",
            "h_t -1.0\n",
            "\n",
            "\n",
            "t= 2\n",
            "h_t-1 -1.0\n",
            "x_t 1\n",
            "h_t -1.0\n",
            "\n",
            "\n",
            "t= 3\n",
            "h_t-1 -1.0\n",
            "x_t -1\n",
            "h_t 0.0\n",
            "\n",
            "\n",
            "t= 4\n",
            "h_t-1 0.0\n",
            "x_t -1\n",
            "h_t 1.0\n",
            "\n",
            "\n",
            "t= 5\n",
            "h_t-1 1.0\n",
            "x_t 1\n",
            "h_t 0.0\n",
            "\n",
            "\n",
            "t= 6\n",
            "h_t-1 0.0\n",
            "x_t 1\n",
            "h_t -1.0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwX3OmtsCnM2",
        "outputId": "3eb530d5-0fa9-4073-d2a2-1a8e22ca7a34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1, -1, 0, 1, 0, -1]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbUJPEH0CqUn",
        "outputId": "ace3aa61-8e51-4867-c59f-0fd498768a11"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1, -1, 0, 1, 0, -1]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        #input_batch = [src len, batch size]\n",
        "        embed = self.dropout(self.embedding(input_batch))\n",
        "        embed = embed.to(device)\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        outputs, (hidden, cell) = self.lstm(embed)\n",
        "\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "hCO7R0wXCriX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_len = 8\n",
        "emb_dim = 10\n",
        "hid_dim=8\n",
        "n_layers=1\n",
        "dropout_prob=0.5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"
      ],
      "metadata": {
        "id": "P2SUH9tYCuMP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_batch = torch.tensor([[0,3,4,2,1]])\n",
        "# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default\n",
        "src_batch = src_batch.t().to(device)\n",
        "print(\"Shape of input(src) tensor:\", src_batch.shape)\n",
        "hidden_t , cell_t = encoder_t(src_batch)\n",
        "print(\"Hidden tensor from encoder:\",hidden_t ,\"\\nCell tensor from encoder:\", cell_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3lb2rfPCv-f",
        "outputId": "f2082462-5299-44b6-f524-246f55fbdfa3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input(src) tensor: torch.Size([5, 1])\n",
            "Hidden tensor from encoder: tensor([[[-0.0335,  0.2564, -0.0671,  0.1655, -0.0365,  0.3264, -0.1172,\n",
            "          -0.1031]]], grad_fn=<StackBackward0>) \n",
            "Cell tensor from encoder: tensor([[[-0.0688,  0.5235, -0.0979,  0.5174, -0.0523,  0.6775, -0.4361,\n",
            "          -0.1516]]], grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "\n",
        "        #input = [batch size]\n",
        "\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        #embedded = [1, batch size, emb dim]\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        prediction_logit = self.fc_out(output.squeeze(0))\n",
        "        prediction = self.softmax(prediction_logit)\n",
        "        #prediction = [batch size, output dim]\n",
        "\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "NQu5WOXpCxl_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 6\n",
        "emb_dim=10\n",
        "hid_dim = 8\n",
        "n_layers=1\n",
        "dropout=0.5\n",
        "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
      ],
      "metadata": {
        "id": "unjzYR-CC0yP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_t = torch.tensor([0]).to(device) #<bos>\n",
        "input_t.shape\n",
        "prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)\n",
        "print(\"Prediction:\", prediction, '\\nHidden:',hidden,'\\nCell:', cell)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZhgmx6bC8D3",
        "outputId": "461755e7-c358-47b6-9f2b-aedf53380809"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[-1.6227, -1.8105, -1.7569, -1.9351, -1.8113, -1.8412]],\n",
            "       grad_fn=<LogSoftmaxBackward0>) \n",
            "Hidden: tensor([[[-0.0451,  0.0018, -0.1890,  0.2388, -0.4348,  0.4971, -0.1448,\n",
            "          -0.2954]]], grad_fn=<StackBackward0>) \n",
            "Cell: tensor([[[-0.1207,  0.0050, -0.3717,  0.4278, -0.7893,  1.0376, -0.2856,\n",
            "          -0.4258]]], grad_fn=<StackBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#trg = [trg len, batch size]\n",
        "#teacher_forcing_ratio is probability to use teacher forcing\n",
        "#e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
        "teacher_forcing_ratio = 0.5\n",
        "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)\n",
        "\n",
        "\n",
        "batch_size = trg.shape[1]\n",
        "trg_len = trg.shape[0]\n",
        "trg_vocab_size = decoder_t.output_dim\n",
        "\n",
        "#tensor to store decoder outputs\n",
        "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
        "\n",
        "#send to device\n",
        "\n",
        "hidden_t = hidden_t.to(device)\n",
        "cell_t = cell_t.to(device)\n",
        "\n",
        "\n",
        "#first input to the decoder is the <bos> tokens\n",
        "input = trg[0,:]\n",
        "\n",
        "\n",
        "for t in range(1, trg_len):\n",
        "\n",
        "    #you loop through the trg len and generate tokens\n",
        "    #decoder receives previous generated token, cell and hidden\n",
        "    # decoder outputs it prediction(probablity distribution for the next token) and updates hidden and cell\n",
        "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
        "\n",
        "    #place predictions in a tensor holding predictions for each token\n",
        "    outputs_t[t] = output_t\n",
        "\n",
        "    #decide if you are going to use teacher forcing or not\n",
        "    teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "    #get the highest predicted token from your predictions\n",
        "    top1 = output_t.argmax(1)\n",
        "\n",
        "\n",
        "    #if teacher forcing, use actual next token as next input\n",
        "    #if not, use predicted token\n",
        "    #input = trg[t] if teacher_force else top1\n",
        "    input = trg[t] if teacher_force else top1\n",
        "\n",
        "print(outputs_t,outputs_t.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D4bWEG1C-Mg",
        "outputId": "3e31b96c-ff7f-4a0e-a137-4e3b0c8d9f25"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-1.6449, -1.8320, -1.7967, -1.9003, -1.7759, -1.8191]],\n",
            "\n",
            "        [[-1.6740, -1.9710, -1.8165, -1.8073, -1.5906, -1.9470]],\n",
            "\n",
            "        [[-1.6352, -1.9336, -1.8781, -1.7639, -1.5915, -2.0204]],\n",
            "\n",
            "        [[-1.6117, -1.9768, -1.9004, -1.8559, -1.5982, -1.8717]]],\n",
            "       grad_fn=<CopySlices>) torch.Size([5, 1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that you need to get the argmax from the second dimension as **outputs** is an array of **output** tensors\n",
        "pred_tokens = outputs_t.argmax(2)\n",
        "print(pred_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT7QJjwCDBKw",
        "outputId": "9c1a4162-6d1b-4c12-b513-88ba67dfe527"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0],\n",
            "        [0],\n",
            "        [4],\n",
            "        [4],\n",
            "        [4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device,trg_vocab):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.trg_vocab = trg_vocab\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n",
        "\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        hidden = hidden.to(device)\n",
        "        cell = cell.to(device)\n",
        "\n",
        "\n",
        "        #first input to the decoder is the <bos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "\n",
        "            #decide if you are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from your predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            #input = trg[t] if teacher_force else top1\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "hZcBHsTEDDLI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Wrap iterator with tqdm for progress logging\n",
        "    train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
        "\n",
        "    for i, (src,trg) in enumerate(iterator):\n",
        "\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "\n",
        "        trg = trg[1:].contiguous().view(-1)\n",
        "\n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tqdm progress bar with the current loss\n",
        "        train_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "    return epoch_loss / len(list(iterator))"
      ],
      "metadata": {
        "id": "YHiw6JrkDFsJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Wrap iterator with tqdm for progress logging\n",
        "    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (src,trg) in enumerate(iterator):\n",
        "\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "\n",
        "            trg = trg[1:].contiguous().view(-1)\n",
        "\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            # Update tqdm progress bar with the current loss\n",
        "            valid_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(list(iterator))"
      ],
      "metadata": {
        "id": "b1AZiU3TDJR5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'"
      ],
      "metadata": {
        "id": "R5D7kHJgDLTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run Multi30K_de_en_dataloader.py"
      ],
      "metadata": {
        "id": "CJUQ5I_nDPNK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)#,flip=True)"
      ],
      "metadata": {
        "id": "tDG65ZKRDTrz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src, trg = next(iter(train_dataloader))\n",
        "src,trg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5qWtACRDZAE",
        "outputId": "ee9a55fa-8c2e-468c-8983-e5f9f2175489"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[    2,     2,     2,     2],\n",
              "         [    3,  5510,  5510, 12642],\n",
              "         [    1,     3,     3,     8],\n",
              "         [    1,     1,     1,  1701],\n",
              "         [    1,     1,     1,     3]]),\n",
              " tensor([[   2,    2,    2,    2],\n",
              "         [   3, 6650,  216,    6],\n",
              "         [   1, 4623,  110, 3398],\n",
              "         [   1,  259, 3913,  202],\n",
              "         [   1,  172, 1650,  109],\n",
              "         [   1, 9953, 3823,   37],\n",
              "         [   1,  115,   71,    3],\n",
              "         [   1,  692, 2808,    1],\n",
              "         [   1, 3428, 2187,    1],\n",
              "         [   1,    5,    5,    1],\n",
              "         [   1,    3,    3,    1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_itr = iter(train_dataloader)\n",
        "# moving forward in the dataset to reach sequences of longer length for illustration purpose. (Remember the dataset is sorted on sequence len for optimal padding)\n",
        "for n in range(1000):\n",
        "    german, english= next(data_itr)\n",
        "\n",
        "for n in range(3):\n",
        "    german, english=next(data_itr)\n",
        "    german=german.T\n",
        "    english=english.T\n",
        "    print(\"________________\")\n",
        "    print(\"german\")\n",
        "    for g in german:\n",
        "        print(index_to_german(g))\n",
        "    print(\"________________\")\n",
        "    print(\"english\")\n",
        "    for e in english:\n",
        "        print(index_to_eng(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-01UDXYDbFb",
        "outputId": "b31bf6b7-bedb-4826-cc0e-7e09127d45dd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "________________\n",
            "german\n",
            "<bos> Personen mit schwarzen Hüten in der Innenstadt . <eos>\n",
            "<bos> Eine Gruppe Menschen protestiert in einer Stadt . <eos>\n",
            "<bos> Eine Gruppe teilt ihre politischen Ansichten mit . <eos>\n",
            "<bos> Mehrere Personen sitzen an einem felsigen Strand . <eos>\n",
            "________________\n",
            "english\n",
            "<bos> People in black hats gathered together downtown . <eos> <pad> <pad> <pad>\n",
            "<bos> A group of people protesting in a city . <eos> <pad> <pad>\n",
            "<bos> A group is letting their political opinion be known . <eos> <pad>\n",
            "<bos> A group of people are sitting on a rocky beach . <eos>\n",
            "________________\n",
            "german\n",
            "<bos> Zwei sitzende Personen mit Hüten und Sonnenbrillen . <eos>\n",
            "<bos> Ein kleiner Junge mit Hut beim Angeln . <eos>\n",
            "<bos> Diese zwei Frauen haben Spaß im Giorgio's . <eos>\n",
            "<bos> Zwei kleine Kinder schlafen auf dem Sofa . <eos>\n",
            "________________\n",
            "english\n",
            "<bos> Two people sitting in hats and shades . <eos> <pad> <pad> <pad>\n",
            "<bos> A young boy in a hat is fishing by himself . <eos>\n",
            "<bos> These two women is at Giorgio 's having fun . <eos> <pad>\n",
            "<bos> Two young children are asleep on a couch . <eos> <pad> <pad>\n",
            "________________\n",
            "german\n",
            "<bos> Zwei junge Mädchen marschieren in einem Umzug . <eos>\n",
            "<bos> Eine Frau läuft vor einer gestreiften Wand . <eos>\n",
            "<bos> Ein Mann fährt Jet-Ski auf dem Ozean . <eos>\n",
            "<bos> Die städtischen Straßenbahnen an einem sonnigen Tag . <eos>\n",
            "________________\n",
            "english\n",
            "<bos> Two young girls walk in a parade . <eos> <pad> <pad> <pad> <pad>\n",
            "<bos> A woman is running in front of a striped wall . <eos> <pad>\n",
            "<bos> A man rides a jet ski across the ocean . <eos> <pad> <pad>\n",
            "<bos> The urban trolly 's of a city on a sunny day . <eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "pPj_8mViDcuT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab_transform['de'])\n",
        "OUTPUT_DIM = len(vocab_transform['en'])\n",
        "ENC_EMB_DIM = 128 #256\n",
        "DEC_EMB_DIM = 128 #256\n",
        "HID_DIM = 256 #512\n",
        "N_LAYERS = 1 #2\n",
        "ENC_DROPOUT = 0.3 #0.5\n",
        "DEC_DROPOUT = 0.3 #0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)"
      ],
      "metadata": {
        "id": "-MMWRF1HDquV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy8foIVLDsNV",
        "outputId": "df0e0c6c-4560-4ecb-bb9b-22176c200789"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19214, 128)\n",
              "    (lstm): LSTM(128, 256, dropout=0.3)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10837, 128)\n",
              "    (lstm): LSTM(128, 256, dropout=0.3)\n",
              "    (fc_out): Linear(in_features=256, out_features=10837, bias=True)\n",
              "    (softmax): LogSoftmax(dim=1)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (trg_vocab): Vocab()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCoGEea_DtzE",
        "outputId": "0b565779-74e6-4228-db53-6aac567a951c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 7,422,165 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "metadata": {
        "id": "7zhHUTgvDyUV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip trainning and run a trainmed model\n",
        "\n",
        "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt'\n",
        "model.load_state_dict(torch.load('RNN-TR-model.pt',map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "uuokRHH4D4G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(device)\n",
        "\n",
        "        # Pass the source tensor through the encoder\n",
        "        hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "        # Create a tensor to store the generated translation\n",
        "        # get_stoi() maps tokens to indices\n",
        "        trg_indexes = [trg_vocab.get_stoi()['<bos>']]  # Start with <bos> token\n",
        "\n",
        "        # Convert the initial token to a PyTorch tensor\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
        "\n",
        "        # Move the tensor to the same device as the model\n",
        "        trg_tensor = trg_tensor.to(model.device)\n",
        "\n",
        "\n",
        "        # Generate the translation\n",
        "        for _ in range(max_len):\n",
        "\n",
        "            # Pass the target tensor and the previous hidden and cell states through the decoder\n",
        "            output, hidden, cell = model.decoder(trg_tensor[-1], hidden, cell)\n",
        "\n",
        "            # Get the predicted next token\n",
        "            pred_token = output.argmax(1)[-1].item()\n",
        "\n",
        "            # Append the predicted token to the translation\n",
        "            trg_indexes.append(pred_token)\n",
        "\n",
        "\n",
        "            # If the predicted token is the <eos> token, stop generating\n",
        "            if pred_token == trg_vocab.get_stoi()['<eos>']:\n",
        "                break\n",
        "\n",
        "            # Convert the predicted token to a PyTorch tensor\n",
        "            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n",
        "\n",
        "            # Move the tensor to the same device as the model\n",
        "            trg_tensor = trg_tensor.to(model.device)\n",
        "\n",
        "        # Convert the generated tokens to text\n",
        "        # get_itos() maps indices to tokens\n",
        "        trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n",
        "\n",
        "        # Remove the <sos> and <eos> from the translation\n",
        "        if trg_tokens[0] == '<bos>':\n",
        "            trg_tokens = trg_tokens[1:]\n",
        "        if trg_tokens[-1] == '<eos>':\n",
        "            trg_tokens = trg_tokens[:-1]\n",
        "\n",
        "        # Return the translation list as a string\n",
        "\n",
        "        translation = \" \".join(trg_tokens)\n",
        "\n",
        "        return translation"
      ],
      "metadata": {
        "id": "CCpLtXSQEVMR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('RNN-TR-model.pt'))\n",
        "\n",
        "# Actual translation: Asian man sweeping the walkway.\n",
        "src_sentence = 'Ein asiatischer Mann kehrt den Gehweg.'\n",
        "\n",
        "\n",
        "generated_translation = generate_translation(model, src_sentence=src_sentence, src_vocab=vocab_transform['de'], trg_vocab=vocab_transform['en'], max_len=12)\n",
        "#generated_translation = \" \".join(generated_translation_list).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n",
        "print(generated_translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXPgIXQFEbTZ",
        "outputId": "f7341d5c-50ac-4fb7-b2b8-99551b132779"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An Asian man is on the sidewalk .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu_score(generated_translation, reference_translations):\n",
        "    # Convert the generated translations and reference translations into the expected format for sentence_bleu\n",
        "    references = [reference.split() for reference in reference_translations]\n",
        "    hypothesis = generated_translation.split()\n",
        "\n",
        "    # Calculate the BLEU score\n",
        "    bleu_score = sentence_bleu(references, hypothesis)\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "4RCwKAYgEgQB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_translations = [\n",
        "    \"Asian man sweeping the walkway .\",\n",
        "    \"An asian man sweeping the walkway .\",\n",
        "    \"An Asian man sweeps the sidewalk .\",\n",
        "    \"An Asian man is sweeping the sidewalk .\",\n",
        "    \"An asian man is sweeping the walkway .\",\n",
        "    \"Asian man sweeping the sidewalk .\"\n",
        "]\n",
        "\n",
        "bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n",
        "print(\"BLEU Score:\", bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYKlz7IWEkDL",
        "outputId": "8c67d655-c6b6-4e62-e0b0-f208d3621120"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "german_text = \"Menschen gehen auf der Straße\"\n",
        "\n",
        "# The function should be defined to accept the text, the model, source and target vocabularies, and the device as parameters.\n",
        "english_translation = generate_translation(\n",
        "    model,\n",
        "    src_sentence=german_text,\n",
        "    src_vocab=vocab_transform['de'],\n",
        "    trg_vocab=vocab_transform['en'],\n",
        "    max_len=50\n",
        ")\n",
        "\n",
        "# Display the original and translated text\n",
        "print(f\"Original German text: {german_text}\")\n",
        "print(f\"Translated English text: {english_translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsoahMxkElpi",
        "outputId": "d51e9484-3ac8-4827-f9dd-dddaba752c4e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original German text: Menschen gehen auf der Straße\n",
            "Translated English text: People are walking on the street .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "63R6_N3VEuUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
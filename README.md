# ü§ñ GenAI & LLMs: Architecture and Data Preparation üöÄ

## üéØ Objective

The primary goal of this repository is to serve as a **comprehensive hands-on guide and experimentation ground** for Generative AI and Large Language Models (LLMs). Based on the curriculum from the IBM course, this project focuses on understanding underlying architectures and the critical data preparation phase required for modern AI engineering.

This project aims to:
* **Explore Generative Architectures:** Implement and differentiate between various models like RNNs, Transformers, GANs, VAEs, and Diffusion models.
* **Master Data Preparation:** Demonstrate professional-grade tokenization and data pipeline construction.
* **Practical LLM Implementation:** Bridge the gap between theoretical knowledge of models (like GPT and BERT) and practical, runnable code examples.
* **Build Efficient Pipelines:** Utilize PyTorch data loaders to prepare high-quality datasets for model training.

---

## üèóÔ∏è Core Architectures Explored

This library documents the implementation and logic behind the following generative paradigms:

* **Transformers:** The backbone of modern LLMs, focusing on self-attention mechanisms (e.g., GPT, BERT).
* **Generative Adversarial Networks (GANs):** Exploring the competition between Generators and Discriminators for synthetic data.
* **Variational Autoencoders (VAEs):** Understanding probabilistic latent space modeling.
* **Diffusion Models:** Implementing the technology powering state-of-the-art image synthesis.
* **RNNs:** Sequential data processing and its evolution into modern attention-based systems.



---

## üíª Technologies Used

* **Python (Version 3.x):** The core programming language.
* **PyTorch:** For building neural network architectures and tensor management.
* **Hugging Face:** For accessing pre-trained models and state-of-the-art tokenizers.
* **NLTK & spaCy:** For advanced Natural Language Processing and text preprocessing.
* **PyTorch DataLoaders:** For building efficient and scalable data pipelines.
* **NumPy & Pandas:** For numerical operations and data manipulation.

---

## üõ†Ô∏è Key Methods & Techniques

| Category | Methods Covered |
| :--- | :--- |
| **Tokenization** | Word-level, Sub-word (BPE), and Character-level tokenization. |
| **Preprocessing** | Text cleaning, stop-word removal, and lemmatization via spaCy/NLTK. |
| **Pipeline Engineering** | Creating custom `Dataset` classes and optimized Batching strategies. |
| **Model Logic** | Implementing Attention mechanisms, Encoder-only, and Decoder-only models. |

---

## üìö Acknowledgments

The examples and architectural logic in this repository are inspired by the **"Generative AI and LLMs: Architecture and Data Preparation"** course by **IBM**, part of the Generative AI Engineering Essentials Professional Certificate.
